{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim\n",
    "\n",
    "- End to End example of classication problem with Titanic dataset in sklearn.\n",
    "- Generate some scripts that can be used for model evaluation in other projects. \n",
    "\n",
    "Notes:\n",
    "- data from: http://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./titanic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.pop('Survived')\n",
    "X = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see imbalance in dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([545, 342], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3855693348365276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "342/(545+342)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass                     0\n",
       "Name                       0\n",
       "Sex                        0\n",
       "Age                        0\n",
       "Siblings/Spouses Aboard    0\n",
       "Parents/Children Aboard    0\n",
       "Fare                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'] = (df.Sex == 'male').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard  \\\n",
       "0       3    1  22.0                        1                        0   \n",
       "1       1    0  38.0                        1                        0   \n",
       "2       3    0  26.0                        0                        0   \n",
       "3       1    0  35.0                        1                        0   \n",
       "4       3    1  35.0                        0                        0   \n",
       "\n",
       "      Fare  \n",
       "0   7.2500  \n",
       "1  71.2833  \n",
       "2   7.9250  \n",
       "3  53.1000  \n",
       "4   8.0500  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make test train split\n",
    "\n",
    "I dont think that *train_test_split* is lookin to keep an even number distribution of classes in each set. But i guess normally your train/test split is large enough that you are likley to get a good representation of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([357, 237], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make basline prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions based on the sex of passenget only; male = survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SexModel():\n",
    "    \"\"\" A model which makes predictions using only the sex of the case. Male = pred of 1.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pred(self, X):\n",
    "        \" when called takes 'Sex' column and returns the series as prediction. X is dataframe which must contain column called 'Sex'\"\n",
    "        return(X['Sex'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SexModel()\n",
    "pred_train = base_model.pred(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get test accuracy for training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2239057239057239"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train,pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25040650406504067"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_train,pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: very low score with this basic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make basic reg model\n",
    "\n",
    "This model does not reuqire hyper-parameter tuning and so no CrossValidation is needed. We just fit to the training data and the model is ready for evaluation onthe test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(C=10000, solver='lbfgs') # high C value makes into normal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr1 = lgr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model eval\n",
    "\n",
    "Using test data to eval model perf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class BinClassEval():\n",
    "    \"\"\" \n",
    "    Makes all evalutation you need to assess binary classifcation models.\n",
    "    Suggested use for final evaluation once models have been decided. e.g. running on test set. \n",
    "    Built for sklearn models: some functionality requires model to have either a predict proba or decision function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, model, X, y, plot_title='Model Evaluation', plot = False):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        -----\n",
    "        Model, sklearn model, (that has had .fit() method called aready).\n",
    "        X, df/numpy array, containing features\n",
    "        y, df/numpy array, containing binary target\n",
    "        \"\"\"\n",
    "        #### assign inputs\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.plot_title = plot_title\n",
    "        \n",
    "        self.proba_pred_avail = False\n",
    "        \n",
    "        #### calc prob and decision functions (where possible). \n",
    "        ### NOTE: these are assigned to the same proba_preds attribute. If predict_prob avail then this takes precident.\n",
    "        if hasattr(model, 'decision_function'):\n",
    "            predDF = model.decision_function(X) # warning, some model dont have DF\n",
    "            self.proba_preds = predDF\n",
    "            self.DF = predDF\n",
    "            print('Model has decision_function.')\n",
    "            \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba_preds = model.predict_proba(X)[:,1]\n",
    "            self.proba_preds = proba_preds\n",
    "            self.proba_preds_avail = True\n",
    "            print('Model has predict_proba.')\n",
    "            \n",
    "        self.label_preds = self.model.predict(X)\n",
    "            \n",
    "        #### run evaluation\n",
    "        self.AUC()\n",
    "        self.F1()\n",
    "        self.accuracy()\n",
    "        \n",
    "        if plot == True:\n",
    "            self.plot_AUC_PR()\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def AUC(self):\n",
    "        \"Prints and returns AUC score.\"\n",
    "        AUC = roc_auc_score(self.y, self.proba_preds).round(3)\n",
    "        self.AUC = AUC\n",
    "        print('AUC: ', AUC) # NOTE: sklearn doc says use prob, if not use decision function.\n",
    "        return\n",
    "    \n",
    "    def F1(self):\n",
    "        \"Prints and returns F1 score.\"\n",
    "        F1 = f1_score(self.y, self.label_preds).round(3)\n",
    "        self.F1 = F1\n",
    "        print('F1 score: ', F1)\n",
    "        return\n",
    "    \n",
    "    def accuracy(self):\n",
    "        \"Prints and returns accuracy score.\"\n",
    "        accuracy = accuracy_score(self.y, self.label_preds).round(3)\n",
    "        self.accuracy = accuracy\n",
    "        print('accuracy: ', accuracy)\n",
    "        return\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        \"Prints confusion matrix.\"\n",
    "        return\n",
    "    \n",
    "    def plot_AUC_PR(self):\n",
    "        \"Plots AUC and Precision-Recall plot as one figures.\"\n",
    "        \n",
    "        precisions, recalls, thresholds = precision_recall_curve(self.y, self.proba_preds)\n",
    "\n",
    "        fpr,tpr,thresholds_ROC = roc_curve(self.y, self.proba_preds)\n",
    "        \n",
    "        fig,ax = plt.subplots(1,2,figsize=(9,4))\n",
    "        fig.suptitle(self.plot_title)\n",
    "        # prec-recall plot\n",
    "        ax[0].plot(recalls[:-1],precisions[:-1],'g-') #,label='Reca')\n",
    "        ax[0].set_ylabel('Precision')\n",
    "        ax[0].set_xlabel('Recall')\n",
    "        ax[0].legend(frameon=True,loc='center right')\n",
    "        ax[0].set_ylim([0,1.1])\n",
    "        # AUC plot\n",
    "        ax[1].plot(fpr,tpr)\n",
    "        ax[1].plot([0,1],[0,1],'k--') # 45def line\n",
    "        ax[1].set_xlabel('F positive rate')\n",
    "        ax[1].set_ylabel('T positve rate')\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has decision_function.\n",
      "Model has predict_proba.\n",
      "AUC:  0.854\n",
      "F1 score:  0.752\n",
      "accuracy:  0.81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BinClassEval at 0x1d4d2705358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinClassEval(lgr1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above evaluation is evaluating the model on the whole training set after it has been fit on that set. It looks to be better practice to use a cross_val score. This fits to each individual set and evaluates on the seperate 'validation' set. An average is taken over each of the folds. This means that each model after traiing is tested on data that has not been used in training. Presumably it therefore gives a better idea of how the model may generalise to new data when it is fitted to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.7945045045045045\n",
      "Std:  0.025328857267352796\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lgr1, X_train, y_train, cv=4, scoring='accuracy')\n",
    "\n",
    "def disp_scores(scores):\n",
    "    \" Prints mean and std of scores.\"\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Std: ', scores.std())\n",
    "    \n",
    "disp_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.7344393207436685\n",
      "Std:  0.018732595805414292\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(lgr1, X_train, y_train, cv=4, scoring='f1')\n",
    "\n",
    "def disp_scores(scores):\n",
    "    \" Prints mean and std of scores.\"\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Std: ', scores.std())\n",
    "    \n",
    "disp_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: accurancy and f1 score have both reduced when we use CV scoring. This is expected as model is less likley to overfit. It is more likely a closer estimate to hte true performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log regression with regularisation\n",
    "\n",
    "If we need to tune hyperparameters then we need to use Cross Validation within the training set.\n",
    "\n",
    "NOTE: dont understand why there is no difference when re-running; there is surely a difference depending on the cv splitting, which should change each time i rerun??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgrr = LogisticRegression(solver='liblinear') # high C value makes into normal logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C':[0.1,1,10,100,1000,10000]}\n",
    "grid_search = GridSearchCV(lgrr, param_grid, scoring='f1', cv=10, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [0.1, 1, 10, 100, 1000, 10000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939070448789496 0.05895751199200433 {'C': 0.1}\n",
      "0.7140921372539883 0.055690204322775244 {'C': 1}\n",
      "0.7380263289463946 0.07047613587559015 {'C': 10}\n",
      "0.7440226395540406 0.07629141689060742 {'C': 100}\n",
      "0.7440226395540406 0.07629141689060742 {'C': 1000}\n",
      "0.7440226395540406 0.07629141689060742 {'C': 10000}\n"
     ]
    }
   ],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, std_test_score, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"], cvres[\"params\"]):\n",
    "    print((mean_score), std_test_score , params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: optimal 'C' is changing when have differnet training and test sets (i.e. random state is not set). Could be a conversion issue of logreg model (based on features not scaled).\n",
    "\n",
    "\n",
    "Perofrmance for each fold data is available for each test and train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00419209, 0.00259681, 0.00248897, 0.00278914, 0.002595  ,\n",
       "        0.00289316]),\n",
       " 'mean_score_time': array([0.00159838, 0.00079763, 0.00099988, 0.00099483, 0.00099699,\n",
       "        0.00129437]),\n",
       " 'mean_test_score': array([0.69390704, 0.71409214, 0.73802633, 0.74402264, 0.74402264,\n",
       "        0.74402264]),\n",
       " 'mean_train_score': array([0.70776571, 0.72309439, 0.74392428, 0.7488709 , 0.74929759,\n",
       "        0.74960654]),\n",
       " 'param_C': masked_array(data=[0.1, 1, 10, 100, 1000, 10000],\n",
       "              mask=[False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100},\n",
       "  {'C': 1000},\n",
       "  {'C': 10000}],\n",
       " 'rank_test_score': array([6, 5, 4, 1, 1, 1]),\n",
       " 'split0_test_score': array([0.66666667, 0.71428571, 0.71428571, 0.71428571, 0.71428571,\n",
       "        0.71428571]),\n",
       " 'split0_train_score': array([0.7025641 , 0.72361809, 0.75425791, 0.75662651, 0.75662651,\n",
       "        0.75662651]),\n",
       " 'split1_test_score': array([0.77272727, 0.79069767, 0.86363636, 0.86363636, 0.86363636,\n",
       "        0.86363636]),\n",
       " 'split1_train_score': array([0.68407311, 0.72040302, 0.74452555, 0.74576271, 0.74576271,\n",
       "        0.74576271]),\n",
       " 'split2_test_score': array([0.63157895, 0.71794872, 0.76190476, 0.76190476, 0.76190476,\n",
       "        0.76190476]),\n",
       " 'split2_train_score': array([0.7025641 , 0.72319202, 0.75609756, 0.76029056, 0.75845411,\n",
       "        0.75845411]),\n",
       " 'split3_test_score': array([0.68085106, 0.71111111, 0.69565217, 0.66666667, 0.66666667,\n",
       "        0.66666667]),\n",
       " 'split3_train_score': array([0.71859296, 0.72592593, 0.74452555, 0.74939173, 0.74939173,\n",
       "        0.74939173]),\n",
       " 'split4_test_score': array([0.75555556, 0.77272727, 0.77272727, 0.77272727, 0.77272727,\n",
       "        0.77272727]),\n",
       " 'split4_train_score': array([0.70437018, 0.71111111, 0.73300971, 0.73965937, 0.74576271,\n",
       "        0.74576271]),\n",
       " 'split5_test_score': array([0.63636364, 0.66666667, 0.69565217, 0.69565217, 0.69565217,\n",
       "        0.69565217]),\n",
       " 'split5_train_score': array([0.71573604, 0.72361809, 0.74201474, 0.74201474, 0.74201474,\n",
       "        0.74201474]),\n",
       " 'split6_test_score': array([0.76      , 0.73076923, 0.79245283, 0.81481481, 0.81481481,\n",
       "        0.81481481]),\n",
       " 'split6_train_score': array([0.70437018, 0.715     , 0.72906404, 0.73529412, 0.73529412,\n",
       "        0.73529412]),\n",
       " 'split7_test_score': array([0.59090909, 0.57777778, 0.58333333, 0.58333333, 0.58333333,\n",
       "        0.58333333]),\n",
       " 'split7_train_score': array([0.72952854, 0.73945409, 0.75609756, 0.76399027, 0.76399027,\n",
       "        0.76399027]),\n",
       " 'split8_test_score': array([0.73170732, 0.74418605, 0.77272727, 0.77272727, 0.77272727,\n",
       "        0.77272727]),\n",
       " 'split8_train_score': array([0.70229008, 0.72180451, 0.74019608, 0.75      , 0.75      ,\n",
       "        0.75      ]),\n",
       " 'split9_test_score': array([0.71111111, 0.71111111, 0.72340426, 0.79166667, 0.79166667,\n",
       "        0.79166667]),\n",
       " 'split9_train_score': array([0.71356784, 0.72681704, 0.73945409, 0.74567901, 0.74567901,\n",
       "        0.74876847]),\n",
       " 'std_fit_time': array([0.00097189, 0.0006603 , 0.00050354, 0.000397  , 0.00049451,\n",
       "        0.00082496]),\n",
       " 'std_score_time': array([4.83090682e-04, 3.98815540e-04, 9.33298352e-06, 1.51384346e-05,\n",
       "        1.26779738e-05, 4.52407091e-04]),\n",
       " 'std_test_score': array([0.05895751, 0.0556902 , 0.07047614, 0.07629142, 0.07629142,\n",
       "        0.07629142]),\n",
       " 'std_train_score': array([0.01164704, 0.00712811, 0.0088395 , 0.00869906, 0.00800721,\n",
       "        0.00792076])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make SVC with model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'C': [100], 'kernel': ['linear']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'kernel':['linear'], 'C':[100]}\n",
    "svc = svm.SVC(gamma=\"scale\")\n",
    "clf2 = GridSearchCV(svc, parameters, cv=3, scoring='f1')\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.712117971242059 0.012689565237110768 {'kernel': 'linear', 'C': 100}\n"
     ]
    }
   ],
   "source": [
    "cvres = clf2.cv_results_\n",
    "for mean_score, std_test_score, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"], cvres[\"params\"]):\n",
    "    print((mean_score), std_test_score , params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has decision_function.\n",
      "AUC:  0.841\n",
      "F1 score:  0.715\n",
      "accuracy:  0.783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BinClassEval at 0x1d4d31e2e48>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinClassEval(clf2, X_train, y_train, 'Training Set Eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.7121179712420589\n",
      "Std:  0.012689565237110768\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf2, X_train, y_train, cv=3, scoring='f1')\n",
    "\n",
    "def disp_scores(scores):\n",
    "    \" Prints mean and std of scores.\"\n",
    "    print('Mean: ', scores.mean())\n",
    "    print('Std: ', scores.std())\n",
    "    \n",
    "disp_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: we would compare the scores and std of each model perofrmance and now pick one or two suitable candidates for evaluation on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLight drop in non-reg log r (0.73)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has decision_function.\n",
      "Model has predict_proba.\n",
      "AUC:  0.861\n",
      "F1 score:  0.724\n",
      "accuracy:  0.812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BinClassEval at 0x1d4d320ddd8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinClassEval(lgr1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularised LogR model sees a drop in f1 from 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has decision_function.\n",
      "Model has predict_proba.\n",
      "AUC:  0.861\n",
      "F1 score:  0.72\n",
      "accuracy:  0.809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BinClassEval at 0x1d4d31e24a8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinClassEval(grid_search, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM sees a slight increase from 0.71:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has decision_function.\n",
      "AUC:  0.846\n",
      "F1 score:  0.726\n",
      "accuracy:  0.812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BinClassEval at 0x1d4d320d860>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinClassEval(clf2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to compare multiple models within training set with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class CompareBinModels():\n",
    "    \"\"\"\n",
    "    Takes list of models and evaluates each on same set of data.\n",
    "    \n",
    "    This may take some time to run as currently retrains each model on each fold(4).\n",
    "    \"\"\"\n",
    "    def __init__(self, models, X, y):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        ------\n",
    "        models, dict, name (string) and sklearn model pairs (that have been .fit()),\n",
    "        X, df/np array , with features,\n",
    "        y, df/np array , with target (binary),\n",
    "        \n",
    "        \"\"\"\n",
    "        #### assign inputs as attributes\n",
    "        self.models = models\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        #### run evals\n",
    "        self.run_evaluations()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def run_evaluations(self):\n",
    "        #### instantiate each model evaluation object\n",
    "        names = []\n",
    "        means = []\n",
    "        stds = []\n",
    "        print('start run_evals')\n",
    "        \n",
    "        for modelname in models:\n",
    "            model = models[modelname]\n",
    "#             print('in loop', model)\n",
    "            \n",
    "            scores = get_cross_validation_scores(model, self.X, self.y, 'f1')\n",
    "            names.append(modelname)\n",
    "            means.append(np.mean(scores))\n",
    "            \n",
    "            stds.append(np.std(scores))\n",
    "            \n",
    "#             print(scores)\n",
    "            \n",
    "#             m_eval_obj = BinClassEval(model, X, y, plot=False) # create model eval object\n",
    "            # save F1 score\n",
    "#             modelevals[modelname] = m_eval_obj # save evaluation object\n",
    "            \n",
    "        #### make model evaluations, scores etc into df\n",
    "        cols = ['model_name', 'mean', 'std']\n",
    "        df = pd.DataFrame([names, means, stds], index=cols).T\n",
    "        self.modelevals = df\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def compare_models(self):\n",
    "        pass\n",
    "    \n",
    "def get_cross_validation_scores(model, X, y, scoring='f1'):\n",
    "    \"Calc cross validation scores.\"\n",
    "    scores = cross_val_score(model, X, y, cv=4, scoring=scoring)\n",
    "    return(scores)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start run_evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "models = {'simple log reg': lgr1, 'reg log reg': grid_search}\n",
    "evals = CompareBinModels(models, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>simple log reg</td>\n",
       "      <td>0.734439</td>\n",
       "      <td>0.0187326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reg log reg</td>\n",
       "      <td>0.731617</td>\n",
       "      <td>0.0222615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name      mean        std\n",
       "0  simple log reg  0.734439  0.0187326\n",
       "1     reg log reg  0.731617  0.0222615"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.modelevals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
